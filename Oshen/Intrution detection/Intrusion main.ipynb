{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import math\n",
    "#from sort import *\n",
    "import cvzone"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(r'C:\\Users\\oshen geenath\\Desktop\\Intrusion\\airport-36510.mp4')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = YOLO(r'C:\\Users\\oshen geenath\\Desktop\\Intrusion\\best.pt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "classNames = ['person']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 13 persons, 775.8ms\n",
      "Speed: 2.5ms preprocess, 775.8ms inference, 9.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 803.5ms\n",
      "Speed: 1.0ms preprocess, 803.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 738.7ms\n",
      "Speed: 1.0ms preprocess, 738.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 740.7ms\n",
      "Speed: 1.0ms preprocess, 740.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 766.5ms\n",
      "Speed: 1.0ms preprocess, 766.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 11 persons, 845.2ms\n",
      "Speed: 1.0ms preprocess, 845.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 792.8ms\n",
      "Speed: 1.4ms preprocess, 792.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 702.4ms\n",
      "Speed: 1.0ms preprocess, 702.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 724.0ms\n",
      "Speed: 0.0ms preprocess, 724.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 708.2ms\n",
      "Speed: 1.0ms preprocess, 708.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 706.0ms\n",
      "Speed: 0.0ms preprocess, 706.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 718.5ms\n",
      "Speed: 1.0ms preprocess, 718.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 744.0ms\n",
      "Speed: 1.0ms preprocess, 744.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 788.6ms\n",
      "Speed: 1.0ms preprocess, 788.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 737.3ms\n",
      "Speed: 1.0ms preprocess, 737.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 709.6ms\n",
      "Speed: 0.0ms preprocess, 709.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 716.3ms\n",
      "Speed: 1.0ms preprocess, 716.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 690.6ms\n",
      "Speed: 0.0ms preprocess, 690.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 697.7ms\n",
      "Speed: 1.0ms preprocess, 697.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 719.1ms\n",
      "Speed: 0.0ms preprocess, 719.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 724.0ms\n",
      "Speed: 2.1ms preprocess, 724.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 707.8ms\n",
      "Speed: 0.0ms preprocess, 707.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 726.9ms\n",
      "Speed: 0.0ms preprocess, 726.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 783.7ms\n",
      "Speed: 1.0ms preprocess, 783.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 749.2ms\n",
      "Speed: 0.0ms preprocess, 749.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 696.8ms\n",
      "Speed: 0.0ms preprocess, 696.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 732.9ms\n",
      "Speed: 1.5ms preprocess, 732.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 694.1ms\n",
      "Speed: 0.0ms preprocess, 694.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 781.9ms\n",
      "Speed: 1.0ms preprocess, 781.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 717.7ms\n",
      "Speed: 1.4ms preprocess, 717.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 721.6ms\n",
      "Speed: 0.0ms preprocess, 721.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 741.2ms\n",
      "Speed: 1.0ms preprocess, 741.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 744.4ms\n",
      "Speed: 1.0ms preprocess, 744.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 689.2ms\n",
      "Speed: 1.5ms preprocess, 689.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 719.7ms\n",
      "Speed: 0.0ms preprocess, 719.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 720.0ms\n",
      "Speed: 0.0ms preprocess, 720.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 19 persons, 710.8ms\n",
      "Speed: 1.0ms preprocess, 710.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 760.4ms\n",
      "Speed: 0.0ms preprocess, 760.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 736.8ms\n",
      "Speed: 1.5ms preprocess, 736.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 887.7ms\n",
      "Speed: 7.7ms preprocess, 887.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 932.8ms\n",
      "Speed: 1.0ms preprocess, 932.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 14 persons, 726.7ms\n",
      "Speed: 0.0ms preprocess, 726.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 746.2ms\n",
      "Speed: 1.0ms preprocess, 746.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 774.9ms\n",
      "Speed: 1.0ms preprocess, 774.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 766.9ms\n",
      "Speed: 1.0ms preprocess, 766.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 742.4ms\n",
      "Speed: 1.0ms preprocess, 742.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 20 persons, 852.0ms\n",
      "Speed: 3.7ms preprocess, 852.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 879.1ms\n",
      "Speed: 1.0ms preprocess, 879.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 755.0ms\n",
      "Speed: 0.0ms preprocess, 755.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 19 persons, 714.7ms\n",
      "Speed: 0.0ms preprocess, 714.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 822.7ms\n",
      "Speed: 18.4ms preprocess, 822.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 766.2ms\n",
      "Speed: 0.0ms preprocess, 766.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 764.4ms\n",
      "Speed: 1.0ms preprocess, 764.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 19 persons, 778.1ms\n",
      "Speed: 0.0ms preprocess, 778.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 20 persons, 806.9ms\n",
      "Speed: 1.0ms preprocess, 806.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 891.7ms\n",
      "Speed: 1.0ms preprocess, 891.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 1019.2ms\n",
      "Speed: 1.0ms preprocess, 1019.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 894.0ms\n",
      "Speed: 1.0ms preprocess, 894.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 855.9ms\n",
      "Speed: 1.0ms preprocess, 855.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 923.3ms\n",
      "Speed: 2.5ms preprocess, 923.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 12 persons, 904.3ms\n",
      "Speed: 1.0ms preprocess, 904.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 834.8ms\n",
      "Speed: 0.0ms preprocess, 834.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 891.6ms\n",
      "Speed: 1.0ms preprocess, 891.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 900.3ms\n",
      "Speed: 1.0ms preprocess, 900.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 855.1ms\n",
      "Speed: 1.0ms preprocess, 855.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 864.0ms\n",
      "Speed: 1.0ms preprocess, 864.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 854.7ms\n",
      "Speed: 1.1ms preprocess, 854.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 938.0ms\n",
      "Speed: 1.0ms preprocess, 938.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 18 persons, 870.0ms\n",
      "Speed: 1.0ms preprocess, 870.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 863.7ms\n",
      "Speed: 1.1ms preprocess, 863.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 25 persons, 895.0ms\n",
      "Speed: 0.0ms preprocess, 895.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 894.5ms\n",
      "Speed: 1.0ms preprocess, 894.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 20 persons, 918.8ms\n",
      "Speed: 0.0ms preprocess, 918.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 19 persons, 825.8ms\n",
      "Speed: 0.9ms preprocess, 825.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 21 persons, 929.3ms\n",
      "Speed: 31.8ms preprocess, 929.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 21 persons, 933.7ms\n",
      "Speed: 1.0ms preprocess, 933.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 17 persons, 950.9ms\n",
      "Speed: 1.0ms preprocess, 950.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 16 persons, 859.1ms\n",
      "Speed: 1.0ms preprocess, 859.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 15 persons, 993.7ms\n",
      "Speed: 1.0ms preprocess, 993.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 13 persons, 847.9ms\n",
      "Speed: 1.0ms preprocess, 847.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 384x640 11 persons, 918.0ms\n",
      "Speed: 1.0ms preprocess, 918.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m success, img \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m      3\u001B[0m results \u001B[38;5;241m=\u001B[39m model(img, stream\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[0;32m      5\u001B[0m     boxes \u001B[38;5;241m=\u001B[39m r\u001B[38;5;241m.\u001B[39mboxes\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m box \u001B[38;5;129;01min\u001B[39;00m boxes:\n\u001B[0;32m      7\u001B[0m         \u001B[38;5;66;03m# Bounding Bboxes\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001B[0m, in \u001B[0;36m_wrap_generator.<locals>.generator_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     33\u001B[0m     \u001B[38;5;66;03m# Issuing `None` to a generator fires it up\u001B[39;00m\n\u001B[0;32m     34\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[1;32m---> 35\u001B[0m         response \u001B[38;5;241m=\u001B[39m \u001B[43mgen\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m             \u001B[38;5;66;03m# Forward the response to our caller and get its next request\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\yolo\\engine\\predictor.py:185\u001B[0m, in \u001B[0;36mBasePredictor.stream_inference\u001B[1;34m(self, source, model)\u001B[0m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;66;03m# inference\u001B[39;00m\n\u001B[0;32m    184\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdt[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 185\u001B[0m     preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maugment\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maugment\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvisualize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    187\u001B[0m \u001B[38;5;66;03m# postprocess\u001B[39;00m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdt[\u001B[38;5;241m2\u001B[39m]:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:312\u001B[0m, in \u001B[0;36mAutoBackend.forward\u001B[1;34m(self, im, augment, visualize)\u001B[0m\n\u001B[0;32m    309\u001B[0m     im \u001B[38;5;241m=\u001B[39m im\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001B[39;00m\n\u001B[0;32m    311\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpt \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnn_module:  \u001B[38;5;66;03m# PyTorch\u001B[39;00m\n\u001B[1;32m--> 312\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(im, augment\u001B[38;5;241m=\u001B[39maugment, visualize\u001B[38;5;241m=\u001B[39mvisualize) \u001B[38;5;28;01mif\u001B[39;00m augment \u001B[38;5;129;01mor\u001B[39;00m visualize \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjit:  \u001B[38;5;66;03m# TorchScript\u001B[39;00m\n\u001B[0;32m    314\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(im)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:203\u001B[0m, in \u001B[0;36mDetectionModel.forward\u001B[1;34m(self, x, augment, profile, visualize)\u001B[0m\n\u001B[0;32m    201\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m augment:\n\u001B[0;32m    202\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_augment(x)  \u001B[38;5;66;03m# augmented inference, None\u001B[39;00m\n\u001B[1;32m--> 203\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_forward_once\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprofile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvisualize\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\tasks.py:58\u001B[0m, in \u001B[0;36mBaseModel._forward_once\u001B[1;34m(self, x, profile, visualize)\u001B[0m\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m profile:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_profile_one_layer(m, x, dt)\n\u001B[1;32m---> 58\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# run\u001B[39;00m\n\u001B[0;32m     59\u001B[0m y\u001B[38;5;241m.\u001B[39mappend(x \u001B[38;5;28;01mif\u001B[39;00m m\u001B[38;5;241m.\u001B[39mi \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)  \u001B[38;5;66;03m# save output\u001B[39;00m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m visualize:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\ultralytics\\nn\\modules.py:300\u001B[0m, in \u001B[0;36mSPPF.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    298\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv1(x)\n\u001B[0;32m    299\u001B[0m y1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm(x)\n\u001B[1;32m--> 300\u001B[0m y2 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mm\u001B[49m\u001B[43m(\u001B[49m\u001B[43my1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    301\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcv2(torch\u001B[38;5;241m.\u001B[39mcat((x, y1, y2, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mm(y2)), \u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001B[0m, in \u001B[0;36mMaxPool2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    165\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor):\n\u001B[1;32m--> 166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    167\u001B[0m \u001B[43m                        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mceil_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    168\u001B[0m \u001B[43m                        \u001B[49m\u001B[43mreturn_indices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreturn_indices\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_jit_internal.py:484\u001B[0m, in \u001B[0;36mboolean_dispatch.<locals>.fn\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_true(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 484\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m if_false(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\functional.py:782\u001B[0m, in \u001B[0;36m_max_pool2d\u001B[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001B[0m\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stride \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    781\u001B[0m     stride \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mannotate(List[\u001B[38;5;28mint\u001B[39m], [])\n\u001B[1;32m--> 782\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_pool2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkernel_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mceil_mode\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img, stream=True)\n",
    "    for r in results:\n",
    "        boxes = r.boxes\n",
    "        for box in boxes:\n",
    "            # Bounding Bboxes\n",
    "            x1, y1, x2, y2 = box.xyxy[0]\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            #cv2.rectangle(img, (x1, y1), (x2, y2), (255,0,255),3)\n",
    "            w, h = x2 - x1, y2 - y1\n",
    "            cvzone.cornerRect(img, (x1, y1, w, h))\n",
    "            #Confidence\n",
    "            conf = math.ceil((box.conf[0] * 100)) / 100\n",
    "            #Class name\n",
    "            cls = int(box.cls[0])\n",
    "\n",
    "            cvzone.putTextRect(img, f'{classNames[cls]} {conf}', (max(0, x1), max(35, y1)), scale=1, thickness=1)\n",
    "\n",
    "    cv2.imshow('Image', img)\n",
    "    cv2.waitKey(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
